{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Dependency-Track is an intelligent Component Analysis platform that allows organizations to identify and reduce risk in the software supply chain. Dependency-Track takes a unique and highly beneficial approach by leveraging the capabilities of Software Bill of Materials (SBOM). This approach provides capabilities that traditional Software Composition Analysis (SCA) solutions cannot achieve. Dependency-Track monitors component usage across all versions of every application in its portfolio in order to proactively identify risk across an organization. The platform has an API-first design and is ideal for use in CI/CD environments. Features Consumes and produces CycloneDX Software Bill of Materials (SBOM) Consumes and produces CycloneDX Vulnerability Exploitability Exchange (VEX) Component support for: Applications Libraries Frameworks Operating systems Containers Firmware Files Hardware Services Tracks component usage across every application in an organizations portfolio Quickly identify what is affected, and where Identifies multiple forms of risk including Components with known vulnerabilities Out-of-date components Modified components License risk More coming soon... Integrates with multiple sources of vulnerability intelligence including: National Vulnerability Database (NVD) GitHub Advisories Sonatype OSS Index VulnDB from Risk Based Security More coming soon. Helps to prioritize mitigation by incorporating support for the Exploit Prediction Scoring System (EPSS) Maintain a private vulnerability database of vulnerability components Robust policy engine with support for global and per-project policies Security risk and compliance License risk and compliance Operational risk and compliance Ecosystem agnostic with built-in repository support for: Cargo (Rust) Composer (PHP) Gems (Ruby) Hex (Erlang/Elixir) Maven (Java) NPM (Javascript) NuGet (.NET) Pypi (Python) More coming soon. Identifies APIs and external service components including: Service provider Endpoint URIs Data classification Directional flow of data Trust boundary traversal Authentication requirements Includes a comprehensive auditing workflow for triaging results Configurable notifications supporting Slack, Microsoft Teams, Mattermost, Webhooks, and Email Supports standardized SPDX license ID\u2019s and tracks license use by component Easy to read metrics for components, projects, and portfolio Native support for Kenna Security, Fortify SSC, ThreadFix, and DefectDojo API-first design facilitates easy integration with other systems API documentation available in OpenAPI format OAuth 2.0 + OpenID Connect (OIDC) support for single sign-on (authN/authZ) Supports internally managed users, Active Directory/LDAP, and API Keys Simple to install and configure. Get up and running in just a few minutes","title":"Home"},{"location":"#introduction","text":"Dependency-Track is an intelligent Component Analysis platform that allows organizations to identify and reduce risk in the software supply chain. Dependency-Track takes a unique and highly beneficial approach by leveraging the capabilities of Software Bill of Materials (SBOM). This approach provides capabilities that traditional Software Composition Analysis (SCA) solutions cannot achieve. Dependency-Track monitors component usage across all versions of every application in its portfolio in order to proactively identify risk across an organization. The platform has an API-first design and is ideal for use in CI/CD environments.","title":"Introduction"},{"location":"#features","text":"Consumes and produces CycloneDX Software Bill of Materials (SBOM) Consumes and produces CycloneDX Vulnerability Exploitability Exchange (VEX) Component support for: Applications Libraries Frameworks Operating systems Containers Firmware Files Hardware Services Tracks component usage across every application in an organizations portfolio Quickly identify what is affected, and where Identifies multiple forms of risk including Components with known vulnerabilities Out-of-date components Modified components License risk More coming soon... Integrates with multiple sources of vulnerability intelligence including: National Vulnerability Database (NVD) GitHub Advisories Sonatype OSS Index VulnDB from Risk Based Security More coming soon. Helps to prioritize mitigation by incorporating support for the Exploit Prediction Scoring System (EPSS) Maintain a private vulnerability database of vulnerability components Robust policy engine with support for global and per-project policies Security risk and compliance License risk and compliance Operational risk and compliance Ecosystem agnostic with built-in repository support for: Cargo (Rust) Composer (PHP) Gems (Ruby) Hex (Erlang/Elixir) Maven (Java) NPM (Javascript) NuGet (.NET) Pypi (Python) More coming soon. Identifies APIs and external service components including: Service provider Endpoint URIs Data classification Directional flow of data Trust boundary traversal Authentication requirements Includes a comprehensive auditing workflow for triaging results Configurable notifications supporting Slack, Microsoft Teams, Mattermost, Webhooks, and Email Supports standardized SPDX license ID\u2019s and tracks license use by component Easy to read metrics for components, projects, and portfolio Native support for Kenna Security, Fortify SSC, ThreadFix, and DefectDojo API-first design facilitates easy integration with other systems API documentation available in OpenAPI format OAuth 2.0 + OpenID Connect (OIDC) support for single sign-on (authN/authZ) Supports internally managed users, Active Directory/LDAP, and API Keys Simple to install and configure. Get up and running in just a few minutes","title":"Features"},{"location":"getting-started/configuration/","text":"API Server The central configuration file application.properties resides in the classpath of the WAR by default. This configuration file controls many performance tuning parameters but is most useful for defining optional external database sources, directory services (LDAP), and proxy settings. For containerized deployments, the properties defined in the configuration file can also be specified as environment variables. All environment variables are upper case with periods (.) replaced with underscores (_). Refer to the Docker instructions for configuration examples using environment variables. Dependency-Track administrators are highly encouraged to create a copy of this file in the Dependency-Track data directory and customize it prior to deploying to production. Warning The default embedded H2 database is designed to quickly evaluate and experiment with Dependency-Track. Do not use the embedded H2 database in production environments. See: Database Support . To start Dependency-Track using custom configuration, add the system property alpine.application.properties when executing. For example: -Dalpine.application.properties = ~/.dependency-track/application.properties Default configuration ############################ Alpine Configuration ########################### # Required # Defines the number of worker threads that the event subsystem will consume. # Events occur asynchronously and are processed by the Event subsystem. This # value should be large enough to handle most production situations without # introducing much delay, yet small enough not to pose additional load on an # already resource-constrained server. # A value of 0 will instruct Alpine to allocate 1 thread per CPU core. This # can further be tweaked using the alpine.worker.thread.multiplier property. # Default value is 0. alpine.worker.threads = 0 # Required # Defines a multiplier that is used to calculate the number of threads used # by the event subsystem. This property is only used when alpine.worker.threads # is set to 0. A machine with 4 cores and a multiplier of 4, will use (at most) # 16 worker threads. Default value is 4. alpine.worker.thread.multiplier = 4 # Required # Defines the path to the data directory. This directory will hold logs, keys, # and any database or index files along with application-specific files or # directories. alpine.data.directory = ~/.dependency-track # Required # Defines the interval (in seconds) to log general heath information. If value # equals 0, watchdog logging will be disabled. alpine.watchdog.logging.interval = 0 # Required # Defines the database mode of operation. Valid choices are: # 'server', 'embedded', and 'external'. # In server mode, the database will listen for connections from remote hosts. # In embedded mode, the system will be more secure and slightly faster. # External mode should be used when utilizing an external database server # (i.e. mysql, postgresql, etc). alpine.database.mode = embedded # Optional # Defines the TCP port to use when the database.mode is set to 'server'. alpine.database.port = 9092 # Required # Specifies the JDBC URL to use when connecting to the database. alpine.database.url = jdbc:h2:~/.dependency-track/db # Required # Specifies the JDBC driver class to use. alpine.database.driver = org.h2.Driver # Optional # Specifies the username to use when authenticating to the database. alpine.database.username = sa # Optional # Specifies the password to use when authenticating to the database. # alpine.database.password= # Optional # Specifies if the database connection pool is enabled. alpine.database.pool.enabled = true # Optional # This property controls the maximum size that the pool is allowed to reach, # including both idle and in-use connections. alpine.database.pool.max.size = 20 # Optional # This property controls the minimum number of idle connections in the pool. # This value should be equal to or less than alpine.database.pool.max.size. # Warning: If the value is less than alpine.database.pool.max.size, # alpine.database.pool.idle.timeout will have no effect. alpine.database.pool.min.idle = 10 # Optional # This property controls the maximum amount of time that a connection is # allowed to sit idle in the pool. alpine.database.pool.idle.timeout = 300000 # Optional # This property controls the maximum lifetime of a connection in the pool. # An in-use connection will never be retired, only when it is closed will # it then be removed. alpine.database.pool.max.lifetime = 600000 # Optional # When authentication is enforced, API keys are required for automation, and # the user interface will prevent anonymous access by prompting for login # credentials. alpine.enforce.authentication = true # Optional # When authorization is enforced, team membership for both API keys and user # accounts are restricted to what the team itself has access to. To enforce # authorization, the enforce.authentication property (above) must be true. alpine.enforce.authorization = true # Required # Specifies the number of bcrypt rounds to use when hashing a users password. # The higher the number the more secure the password, at the expense of # hardware resources and additional time to generate the hash. alpine.bcrypt.rounds = 14 # Required # Defines if LDAP will be used for user authentication. If enabled, # alpine.ldap.* properties should be set accordingly. alpine.ldap.enabled = false # Optional # Specifies the LDAP server URL # Example (Microsoft Active Directory): # alpine.ldap.server.url=ldap://ldap.example.com:3268 # alpine.ldap.server.url=ldaps://ldap.example.com:3269 # Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc): # alpine.ldap.server.url=ldap://ldap.example.com:389 # alpine.ldap.server.url=ldaps://ldap.example.com:636 alpine.ldap.server.url = ldap://ldap.example.com:389 # Optional # Specifies the base DN that all queries should search from alpine.ldap.basedn = dc=example,dc=com # Optional # Specifies the LDAP security authentication level to use. Its value is one of # the following strings: \"none\", \"simple\", \"strong\". If this property is empty # or unspecified, the behaviour is determined by the service provider. alpine.ldap.security.auth = simple # Optional # If anonymous access is not permitted, specify a username with limited access # to the directory, just enough to perform searches. This should be the fully # qualified DN of the user. alpine.ldap.bind.username = # Optional # If anonymous access is not permitted, specify a password for the username # used to bind. alpine.ldap.bind.password = # Optional # Specifies if the username entered during login needs to be formatted prior # to asserting credentials against the directory. For Active Directory, the # userPrincipal attribute typically ends with the domain, whereas the # samAccountName attribute and other directory server implementations do not. # The %s variable will be substitued with the username asserted during login. # Example (Microsoft Active Directory): # alpine.ldap.auth.username.format=%s@example.com # Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc): # alpine.ldap.auth.username.format=%s alpine.ldap.auth.username.format = %s@example.com # Optional # Specifies the Attribute that identifies a users ID # Example (Microsoft Active Directory): # alpine.ldap.attribute.name=userPrincipalName # Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc): # alpine.ldap.attribute.name=uid alpine.ldap.attribute.name = userPrincipalName # Optional # Specifies the LDAP attribute used to store a users email address alpine.ldap.attribute.mail = mail # Optional # Specifies the LDAP search filter used to retrieve all groups from the # directory. # Example (Microsoft Active Directory): # alpine.ldap.groups.filter=(&(objectClass=group)(objectCategory=Group)) # Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc): # alpine.ldap.groups.filter=(&(objectClass=groupOfUniqueNames)) alpine.ldap.groups.filter = (&(objectClass=group)(objectCategory=Group)) # Optional # Specifies the LDAP search filter to use to query a user and retrieve a list # of groups the user is a member of. The {USER_DN} variable will be substituted # with the actual value of the users DN at runtime. # Example (Microsoft Active Directory): # alpine.ldap.user.groups.filter=(&(objectClass=group)(objectCategory=Group)(member={USER_DN})) # Example (Microsoft Active Directory - with nested group support): # alpine.ldap.user.groups.filter=(member:1.2.840.113556.1.4.1941:={USER_DN}) # Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc): # alpine.ldap.user.groups.filter=(&(objectClass=groupOfUniqueNames)(uniqueMember={USER_DN})) alpine.ldap.user.groups.filter = (member:1.2.840.113556.1.4.1941:={USER_DN}) # Optional # Specifies the LDAP search filter used to search for groups by their name. # The {SEARCH_TERM} variable will be substituted at runtime. # Example (Microsoft Active Directory): # alpine.ldap.groups.search.filter=(&(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*)) # Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc): # alpine.ldap.groups.search.filter=(&(objectClass=groupOfUniqueNames)(cn=*{SEARCH_TERM}*)) alpine.ldap.groups.search.filter = (&(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*)) # Optional # Specifies the LDAP search filter used to search for users by their name. # The {SEARCH_TERM} variable will be substituted at runtime. # Example (Microsoft Active Directory): # alpine.ldap.users.search.filter=(&(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*)) # Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc): # alpine.ldap.users.search.filter=(&(objectClass=inetOrgPerson)(cn=*{SEARCH_TERM}*)) alpine.ldap.users.search.filter = (&(objectClass=user)(objectCategory=Person)(cn=*{SEARCH_TERM}*)) # Optional # Specifies if mapped LDAP accounts are automatically created upon successful # authentication. When a user logs in with valid credentials but an account has # not been previously provisioned, an authentication failure will be returned. # This allows admins to control specifically which ldap users can access the # system and which users cannot. When this value is set to true, a local ldap # user will be created and mapped to the ldap account automatically. This # automatic provisioning only affects authentication, not authorization. alpine.ldap.user.provisioning = false # Optional # This option will ensure that team memberships for LDAP users are dynamic and # synchronized with membership of LDAP groups. When a team is mapped to an LDAP # group, all local LDAP users will automatically be assigned to the team if # they are a member of the group the team is mapped to. If the user is later # removed from the LDAP group, they will also be removed from the team. This # option provides the ability to dynamically control user permissions via an # external directory. alpine.ldap.team.synchronization = false # Optional # HTTP proxy. If the address is set, then the port must be set too. # alpine.http.proxy.address=proxy.example.com # alpine.http.proxy.port=8888 # alpine.http.proxy.username= # alpine.http.proxy.password= # alpine.no.proxy=localhost,127.0.0.1 # Optional # HTTP Outbound Connection Timeout Settings. All values are in seconds. # alpine.http.timeout.connection=30 # alpine.http.timeout.socket=30 # alpine.http.timeout.pool=60 # Optional # Cross-Origin Resource Sharing (CORS) headers to include in REST responses. # If 'alpine.cors.enabled' is true, CORS headers will be sent, if false, no # CORS headers will be sent. # See Also: https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS # The following are default values #alpine.cors.enabled=true #alpine.cors.allow.origin=* #alpine.cors.allow.methods=GET, POST, PUT, DELETE, OPTIONS #alpine.cors.allow.headers=Origin, Content-Type, Authorization, X-Requested-With, Content-Length, Accept, Origin, X-Api-Key, X-Total-Count, * #alpine.cors.expose.headers=Origin, Content-Type, Authorization, X-Requested-With, Content-Length, Accept, Origin, X-Api-Key, X-Total-Count #alpine.cors.allow.credentials=true #alpine.cors.max.age=3600 # Optional # Defines whether Prometheus metrics will be exposed. # If enabled, metrics will be available via the /metrics endpoint. # This endpoint is NOT subject to access control. If protection is desired, # it must be enforced in the reverse proxy using basic auth. # See also: https://prometheus.io/docs/guides/basic-auth/ alpine.metrics.enabled = false # Required # Defines if OpenID Connect will be used for user authentication. # If enabled, alpine.oidc.* properties should be set accordingly. alpine.oidc.enabled = false # Optional # Defines the issuer URL to be used for OpenID Connect. # This issuer MUST support provider configuration via the /.well-known/openid-configuration endpoint. # See also: # - https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderMetadata # - https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfig alpine.oidc.issuer = # Optional # Defines the name of the claim that contains the username in the provider's userinfo endpoint. # Common claims are \"name\", \"username\", \"preferred_username\" or \"nickname\". # See also: https://openid.net/specs/openid-connect-core-1_0.html#UserInfoResponse alpine.oidc.username.claim = name # Optional # Specifies if mapped OpenID Connect accounts are automatically created upon successful # authentication. When a user logs in with a valid access token but an account has # not been previously provisioned, an authentication failure will be returned. # This allows admins to control specifically which OpenID Connect users can access the # system and which users cannot. When this value is set to true, a local OpenID Connect # user will be created and mapped to the OpenID Connect account automatically. This # automatic provisioning only affects authentication, not authorization. alpine.oidc.user.provisioning = false # Optional # This option will ensure that team memberships for OpenID Connect users are dynamic and # synchronized with membership of OpenID Connect groups or assigned roles. When a team is # mapped to an OpenID Connect group, all local OpenID Connect users will automatically be # assigned to the team if they are a member of the group the team is mapped to. If the user # is later removed from the OpenID Connect group, they will also be removed from the team. This # option provides the ability to dynamically control user permissions via the identity provider. # Note that team synchronization is only performed during user provisioning and after successful # authentication. alpine.oidc.team.synchronization = false # Optional # Defines the name of the claim that contains group memberships or role assignments in the provider's userinfo endpoint. # The claim must be an array of strings. Most public identity providers do not support group or role management. # When using a customizable / on-demand hosted identity provider, name, content, and inclusion in the userinfo endpoint # will most likely need to be configured. alpine.oidc.teams.claim = groups Proxy Configuration Proxy support can be configured in one of two ways, using the proxy settings defined in application.properties or through environment variables. By default, the system will attempt to read the https_proxy , http_proxy and no_proxy environment variables. If one of these are set, Dependency-Track will use them automatically. no_proxy specifies URLs that should be excluded from proxying. This can be a comma-separated list of hostnames, domain names, or a mixture of both. If a port number is specified for a URL, only the requests with that port number to that URL will be excluded from proxying. no_proxy can also set to be a single asterisk ('*') to match all hosts. Dependency-Track supports proxies that require BASIC, DIGEST, and NTLM authentication. Logging Levels Logging levels (INFO, WARN, ERROR, DEBUG, TRACE) can be specified by passing the level to the dependencyTrack.logging.level system property on startup. For example, the following command will start Dependency-Track (embedded) with DEBUG logging: java -Xmx4G -DdependencyTrack.logging.level = DEBUG -jar dependency-track-embedded.war For Docker deployments, simply set the LOGGING_LEVEL environment variable to one of INFO, WARN, ERROR, DEBUG, or TRACE. Frontend The frontend uses a static config.json file that is dynamically requested and evaluated via AJAX. This file resides in <BASE_URL>/static/config.json . Default configuration { // Required // The base URL of the API server. // NOTE: // * This URL must be reachable by the browsers of your users. // * The frontend container itself does NOT communicate with the API server directly, it just serves static files. // * When deploying to dedicated servers, please use the external IP or domain of the API server. \"API_BASE_URL\" : \"\" , // Optional // Defines the issuer URL to be used for OpenID Connect. // See alpine.oidc.issuer property of the backend. \"OIDC_ISSUER\" : \"\" , // Optional // Defines the client ID for OpenID Connect. \"OIDC_CLIENT_ID\" : \"\" , // Optional // Defines the scopes to request for OpenID Connect. // See also: https://openid.net/specs/openid-connect-basic-1_0.html#Scopes \"OIDC_SCOPE\" : \"openid profile email\" , // Optional // Specifies the OpenID Connect flow to use. // Values other than \"implicit\" will result in the Code+PKCE flow to be used. // Usage of the implicit flow is strongly discouraged, but may be necessary when // the IdP of choice does not support the Code+PKCE flow. // See also: // - https://oauth.net/2/grant-types/implicit/ // - https://oauth.net/2/pkce/ \"OIDC_FLOW\" : \"\" , // Optional // Defines the text of the OpenID Connect login button. \"OIDC_LOGIN_BUTTON_TEXT\" : \"\" } For containerized deployments, these settings can be overridden by either: mounting a customized config.json to /app/static/config.json inside the container providing them as environment variables The names of the environment variables are equivalent to their counterparts in config.json . A mounted config.json takes precedence over environment variables. If both are provided, environment variables will be ignored.","title":"Configuration"},{"location":"getting-started/configuration/#api-server","text":"The central configuration file application.properties resides in the classpath of the WAR by default. This configuration file controls many performance tuning parameters but is most useful for defining optional external database sources, directory services (LDAP), and proxy settings. For containerized deployments, the properties defined in the configuration file can also be specified as environment variables. All environment variables are upper case with periods (.) replaced with underscores (_). Refer to the Docker instructions for configuration examples using environment variables. Dependency-Track administrators are highly encouraged to create a copy of this file in the Dependency-Track data directory and customize it prior to deploying to production. Warning The default embedded H2 database is designed to quickly evaluate and experiment with Dependency-Track. Do not use the embedded H2 database in production environments. See: Database Support . To start Dependency-Track using custom configuration, add the system property alpine.application.properties when executing. For example: -Dalpine.application.properties = ~/.dependency-track/application.properties","title":"API Server"},{"location":"getting-started/configuration/#default-configuration","text":"############################ Alpine Configuration ########################### # Required # Defines the number of worker threads that the event subsystem will consume. # Events occur asynchronously and are processed by the Event subsystem. This # value should be large enough to handle most production situations without # introducing much delay, yet small enough not to pose additional load on an # already resource-constrained server. # A value of 0 will instruct Alpine to allocate 1 thread per CPU core. This # can further be tweaked using the alpine.worker.thread.multiplier property. # Default value is 0. alpine.worker.threads = 0 # Required # Defines a multiplier that is used to calculate the number of threads used # by the event subsystem. This property is only used when alpine.worker.threads # is set to 0. A machine with 4 cores and a multiplier of 4, will use (at most) # 16 worker threads. Default value is 4. alpine.worker.thread.multiplier = 4 # Required # Defines the path to the data directory. This directory will hold logs, keys, # and any database or index files along with application-specific files or # directories. alpine.data.directory = ~/.dependency-track # Required # Defines the interval (in seconds) to log general heath information. If value # equals 0, watchdog logging will be disabled. alpine.watchdog.logging.interval = 0 # Required # Defines the database mode of operation. Valid choices are: # 'server', 'embedded', and 'external'. # In server mode, the database will listen for connections from remote hosts. # In embedded mode, the system will be more secure and slightly faster. # External mode should be used when utilizing an external database server # (i.e. mysql, postgresql, etc). alpine.database.mode = embedded # Optional # Defines the TCP port to use when the database.mode is set to 'server'. alpine.database.port = 9092 # Required # Specifies the JDBC URL to use when connecting to the database. alpine.database.url = jdbc:h2:~/.dependency-track/db # Required # Specifies the JDBC driver class to use. alpine.database.driver = org.h2.Driver # Optional # Specifies the username to use when authenticating to the database. alpine.database.username = sa # Optional # Specifies the password to use when authenticating to the database. # alpine.database.password= # Optional # Specifies if the database connection pool is enabled. alpine.database.pool.enabled = true # Optional # This property controls the maximum size that the pool is allowed to reach, # including both idle and in-use connections. alpine.database.pool.max.size = 20 # Optional # This property controls the minimum number of idle connections in the pool. # This value should be equal to or less than alpine.database.pool.max.size. # Warning: If the value is less than alpine.database.pool.max.size, # alpine.database.pool.idle.timeout will have no effect. alpine.database.pool.min.idle = 10 # Optional # This property controls the maximum amount of time that a connection is # allowed to sit idle in the pool. alpine.database.pool.idle.timeout = 300000 # Optional # This property controls the maximum lifetime of a connection in the pool. # An in-use connection will never be retired, only when it is closed will # it then be removed. alpine.database.pool.max.lifetime = 600000 # Optional # When authentication is enforced, API keys are required for automation, and # the user interface will prevent anonymous access by prompting for login # credentials. alpine.enforce.authentication = true # Optional # When authorization is enforced, team membership for both API keys and user # accounts are restricted to what the team itself has access to. To enforce # authorization, the enforce.authentication property (above) must be true. alpine.enforce.authorization = true # Required # Specifies the number of bcrypt rounds to use when hashing a users password. # The higher the number the more secure the password, at the expense of # hardware resources and additional time to generate the hash. alpine.bcrypt.rounds = 14 # Required # Defines if LDAP will be used for user authentication. If enabled, # alpine.ldap.* properties should be set accordingly. alpine.ldap.enabled = false # Optional # Specifies the LDAP server URL # Example (Microsoft Active Directory): # alpine.ldap.server.url=ldap://ldap.example.com:3268 # alpine.ldap.server.url=ldaps://ldap.example.com:3269 # Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc): # alpine.ldap.server.url=ldap://ldap.example.com:389 # alpine.ldap.server.url=ldaps://ldap.example.com:636 alpine.ldap.server.url = ldap://ldap.example.com:389 # Optional # Specifies the base DN that all queries should search from alpine.ldap.basedn = dc=example,dc=com # Optional # Specifies the LDAP security authentication level to use. Its value is one of # the following strings: \"none\", \"simple\", \"strong\". If this property is empty # or unspecified, the behaviour is determined by the service provider. alpine.ldap.security.auth = simple # Optional # If anonymous access is not permitted, specify a username with limited access # to the directory, just enough to perform searches. This should be the fully # qualified DN of the user. alpine.ldap.bind.username = # Optional # If anonymous access is not permitted, specify a password for the username # used to bind. alpine.ldap.bind.password = # Optional # Specifies if the username entered during login needs to be formatted prior # to asserting credentials against the directory. For Active Directory, the # userPrincipal attribute typically ends with the domain, whereas the # samAccountName attribute and other directory server implementations do not. # The %s variable will be substitued with the username asserted during login. # Example (Microsoft Active Directory): # alpine.ldap.auth.username.format=%s@example.com # Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc): # alpine.ldap.auth.username.format=%s alpine.ldap.auth.username.format = %s@example.com # Optional # Specifies the Attribute that identifies a users ID # Example (Microsoft Active Directory): # alpine.ldap.attribute.name=userPrincipalName # Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc): # alpine.ldap.attribute.name=uid alpine.ldap.attribute.name = userPrincipalName # Optional # Specifies the LDAP attribute used to store a users email address alpine.ldap.attribute.mail = mail # Optional # Specifies the LDAP search filter used to retrieve all groups from the # directory. # Example (Microsoft Active Directory): # alpine.ldap.groups.filter=(&(objectClass=group)(objectCategory=Group)) # Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc): # alpine.ldap.groups.filter=(&(objectClass=groupOfUniqueNames)) alpine.ldap.groups.filter = (&(objectClass=group)(objectCategory=Group)) # Optional # Specifies the LDAP search filter to use to query a user and retrieve a list # of groups the user is a member of. The {USER_DN} variable will be substituted # with the actual value of the users DN at runtime. # Example (Microsoft Active Directory): # alpine.ldap.user.groups.filter=(&(objectClass=group)(objectCategory=Group)(member={USER_DN})) # Example (Microsoft Active Directory - with nested group support): # alpine.ldap.user.groups.filter=(member:1.2.840.113556.1.4.1941:={USER_DN}) # Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc): # alpine.ldap.user.groups.filter=(&(objectClass=groupOfUniqueNames)(uniqueMember={USER_DN})) alpine.ldap.user.groups.filter = (member:1.2.840.113556.1.4.1941:={USER_DN}) # Optional # Specifies the LDAP search filter used to search for groups by their name. # The {SEARCH_TERM} variable will be substituted at runtime. # Example (Microsoft Active Directory): # alpine.ldap.groups.search.filter=(&(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*)) # Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc): # alpine.ldap.groups.search.filter=(&(objectClass=groupOfUniqueNames)(cn=*{SEARCH_TERM}*)) alpine.ldap.groups.search.filter = (&(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*)) # Optional # Specifies the LDAP search filter used to search for users by their name. # The {SEARCH_TERM} variable will be substituted at runtime. # Example (Microsoft Active Directory): # alpine.ldap.users.search.filter=(&(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*)) # Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc): # alpine.ldap.users.search.filter=(&(objectClass=inetOrgPerson)(cn=*{SEARCH_TERM}*)) alpine.ldap.users.search.filter = (&(objectClass=user)(objectCategory=Person)(cn=*{SEARCH_TERM}*)) # Optional # Specifies if mapped LDAP accounts are automatically created upon successful # authentication. When a user logs in with valid credentials but an account has # not been previously provisioned, an authentication failure will be returned. # This allows admins to control specifically which ldap users can access the # system and which users cannot. When this value is set to true, a local ldap # user will be created and mapped to the ldap account automatically. This # automatic provisioning only affects authentication, not authorization. alpine.ldap.user.provisioning = false # Optional # This option will ensure that team memberships for LDAP users are dynamic and # synchronized with membership of LDAP groups. When a team is mapped to an LDAP # group, all local LDAP users will automatically be assigned to the team if # they are a member of the group the team is mapped to. If the user is later # removed from the LDAP group, they will also be removed from the team. This # option provides the ability to dynamically control user permissions via an # external directory. alpine.ldap.team.synchronization = false # Optional # HTTP proxy. If the address is set, then the port must be set too. # alpine.http.proxy.address=proxy.example.com # alpine.http.proxy.port=8888 # alpine.http.proxy.username= # alpine.http.proxy.password= # alpine.no.proxy=localhost,127.0.0.1 # Optional # HTTP Outbound Connection Timeout Settings. All values are in seconds. # alpine.http.timeout.connection=30 # alpine.http.timeout.socket=30 # alpine.http.timeout.pool=60 # Optional # Cross-Origin Resource Sharing (CORS) headers to include in REST responses. # If 'alpine.cors.enabled' is true, CORS headers will be sent, if false, no # CORS headers will be sent. # See Also: https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS # The following are default values #alpine.cors.enabled=true #alpine.cors.allow.origin=* #alpine.cors.allow.methods=GET, POST, PUT, DELETE, OPTIONS #alpine.cors.allow.headers=Origin, Content-Type, Authorization, X-Requested-With, Content-Length, Accept, Origin, X-Api-Key, X-Total-Count, * #alpine.cors.expose.headers=Origin, Content-Type, Authorization, X-Requested-With, Content-Length, Accept, Origin, X-Api-Key, X-Total-Count #alpine.cors.allow.credentials=true #alpine.cors.max.age=3600 # Optional # Defines whether Prometheus metrics will be exposed. # If enabled, metrics will be available via the /metrics endpoint. # This endpoint is NOT subject to access control. If protection is desired, # it must be enforced in the reverse proxy using basic auth. # See also: https://prometheus.io/docs/guides/basic-auth/ alpine.metrics.enabled = false # Required # Defines if OpenID Connect will be used for user authentication. # If enabled, alpine.oidc.* properties should be set accordingly. alpine.oidc.enabled = false # Optional # Defines the issuer URL to be used for OpenID Connect. # This issuer MUST support provider configuration via the /.well-known/openid-configuration endpoint. # See also: # - https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderMetadata # - https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfig alpine.oidc.issuer = # Optional # Defines the name of the claim that contains the username in the provider's userinfo endpoint. # Common claims are \"name\", \"username\", \"preferred_username\" or \"nickname\". # See also: https://openid.net/specs/openid-connect-core-1_0.html#UserInfoResponse alpine.oidc.username.claim = name # Optional # Specifies if mapped OpenID Connect accounts are automatically created upon successful # authentication. When a user logs in with a valid access token but an account has # not been previously provisioned, an authentication failure will be returned. # This allows admins to control specifically which OpenID Connect users can access the # system and which users cannot. When this value is set to true, a local OpenID Connect # user will be created and mapped to the OpenID Connect account automatically. This # automatic provisioning only affects authentication, not authorization. alpine.oidc.user.provisioning = false # Optional # This option will ensure that team memberships for OpenID Connect users are dynamic and # synchronized with membership of OpenID Connect groups or assigned roles. When a team is # mapped to an OpenID Connect group, all local OpenID Connect users will automatically be # assigned to the team if they are a member of the group the team is mapped to. If the user # is later removed from the OpenID Connect group, they will also be removed from the team. This # option provides the ability to dynamically control user permissions via the identity provider. # Note that team synchronization is only performed during user provisioning and after successful # authentication. alpine.oidc.team.synchronization = false # Optional # Defines the name of the claim that contains group memberships or role assignments in the provider's userinfo endpoint. # The claim must be an array of strings. Most public identity providers do not support group or role management. # When using a customizable / on-demand hosted identity provider, name, content, and inclusion in the userinfo endpoint # will most likely need to be configured. alpine.oidc.teams.claim = groups","title":"Default configuration"},{"location":"getting-started/configuration/#proxy-configuration","text":"Proxy support can be configured in one of two ways, using the proxy settings defined in application.properties or through environment variables. By default, the system will attempt to read the https_proxy , http_proxy and no_proxy environment variables. If one of these are set, Dependency-Track will use them automatically. no_proxy specifies URLs that should be excluded from proxying. This can be a comma-separated list of hostnames, domain names, or a mixture of both. If a port number is specified for a URL, only the requests with that port number to that URL will be excluded from proxying. no_proxy can also set to be a single asterisk ('*') to match all hosts. Dependency-Track supports proxies that require BASIC, DIGEST, and NTLM authentication.","title":"Proxy Configuration"},{"location":"getting-started/configuration/#logging-levels","text":"Logging levels (INFO, WARN, ERROR, DEBUG, TRACE) can be specified by passing the level to the dependencyTrack.logging.level system property on startup. For example, the following command will start Dependency-Track (embedded) with DEBUG logging: java -Xmx4G -DdependencyTrack.logging.level = DEBUG -jar dependency-track-embedded.war For Docker deployments, simply set the LOGGING_LEVEL environment variable to one of INFO, WARN, ERROR, DEBUG, or TRACE.","title":"Logging Levels"},{"location":"getting-started/configuration/#frontend","text":"The frontend uses a static config.json file that is dynamically requested and evaluated via AJAX. This file resides in <BASE_URL>/static/config.json .","title":"Frontend"},{"location":"getting-started/configuration/#default-configuration_1","text":"{ // Required // The base URL of the API server. // NOTE: // * This URL must be reachable by the browsers of your users. // * The frontend container itself does NOT communicate with the API server directly, it just serves static files. // * When deploying to dedicated servers, please use the external IP or domain of the API server. \"API_BASE_URL\" : \"\" , // Optional // Defines the issuer URL to be used for OpenID Connect. // See alpine.oidc.issuer property of the backend. \"OIDC_ISSUER\" : \"\" , // Optional // Defines the client ID for OpenID Connect. \"OIDC_CLIENT_ID\" : \"\" , // Optional // Defines the scopes to request for OpenID Connect. // See also: https://openid.net/specs/openid-connect-basic-1_0.html#Scopes \"OIDC_SCOPE\" : \"openid profile email\" , // Optional // Specifies the OpenID Connect flow to use. // Values other than \"implicit\" will result in the Code+PKCE flow to be used. // Usage of the implicit flow is strongly discouraged, but may be necessary when // the IdP of choice does not support the Code+PKCE flow. // See also: // - https://oauth.net/2/grant-types/implicit/ // - https://oauth.net/2/pkce/ \"OIDC_FLOW\" : \"\" , // Optional // Defines the text of the OpenID Connect login button. \"OIDC_LOGIN_BUTTON_TEXT\" : \"\" } For containerized deployments, these settings can be overridden by either: mounting a customized config.json to /app/static/config.json inside the container providing them as environment variables The names of the environment variables are equivalent to their counterparts in config.json . A mounted config.json takes precedence over environment variables. If both are provided, environment variables will be ignored.","title":"Default configuration"},{"location":"getting-started/database-support/","text":"Dependency-Track includes an embedded H2 database enabled by default. The intended purpose of this database is for quick evaluation, testing, and demonstration of the platform and its capabilities. Warning The embedded H2 database is not intended for production use. Dependency-Track supports the following database servers: Microsoft SQL Server 2012 and higher MySQL 5.6 and 5.7 PostgreSQL 9.0 and higher To change database settings, edit application.properties found in the Dependency-Track data directory. Examples PostgreSQL Microsoft SQL Server MySQL alpine.database.mode = external alpine.database.url = jdbc:postgresql://localhost:5432/dtrack alpine.database.driver = org.postgresql.Driver alpine.database.username = dtrack alpine.database.password = password alpine.database.mode = external alpine.database.url = jdbc:sqlserver://localhost:1433;databaseName=dtrack;sendStringParametersAsUnicode=false alpine.database.driver = com.microsoft.sqlserver.jdbc.SQLServerDriver alpine.database.username = dtrack alpine.database.password = password alpine.database.mode = external alpine.database.url = jdbc:mysql://localhost:3306/dtrack?autoReconnect=true&useSSL=false alpine.database.driver = com.mysql.jdbc.Driver alpine.database.username = dtrack alpine.database.password = password Warning For MySQL, it is necessary to remove NO_ZERO_IN_DATE and NO_ZERO_DATE from the sql-mode prior to creating the Dependency-Track database. It's also necessary to add ANSI_QUOTES to the sql-mode. Refer to the MySQL documentation for details. There are several ways to change this configuration, however the recommended way is to modify the MySQL configuration (typically my.ini or similar) with the following: [mysqld] sql_mode = \"ANSI_QUOTES,STRICT_TRANS_TABLES,ONLY_FULL_GROUP_BY,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION\" MySQL will erroneously report index key length violations (\"Specified key was too long\"), when infact the multi-byte key length is lower than the actual value. If UTF-8 support is required, do not use MySQL .","title":"Database Support"},{"location":"getting-started/database-support/#examples","text":"PostgreSQL Microsoft SQL Server MySQL alpine.database.mode = external alpine.database.url = jdbc:postgresql://localhost:5432/dtrack alpine.database.driver = org.postgresql.Driver alpine.database.username = dtrack alpine.database.password = password alpine.database.mode = external alpine.database.url = jdbc:sqlserver://localhost:1433;databaseName=dtrack;sendStringParametersAsUnicode=false alpine.database.driver = com.microsoft.sqlserver.jdbc.SQLServerDriver alpine.database.username = dtrack alpine.database.password = password alpine.database.mode = external alpine.database.url = jdbc:mysql://localhost:3306/dtrack?autoReconnect=true&useSSL=false alpine.database.driver = com.mysql.jdbc.Driver alpine.database.username = dtrack alpine.database.password = password Warning For MySQL, it is necessary to remove NO_ZERO_IN_DATE and NO_ZERO_DATE from the sql-mode prior to creating the Dependency-Track database. It's also necessary to add ANSI_QUOTES to the sql-mode. Refer to the MySQL documentation for details. There are several ways to change this configuration, however the recommended way is to modify the MySQL configuration (typically my.ini or similar) with the following: [mysqld] sql_mode = \"ANSI_QUOTES,STRICT_TRANS_TABLES,ONLY_FULL_GROUP_BY,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION\" MySQL will erroneously report index key length violations (\"Specified key was too long\"), when infact the multi-byte key length is lower than the actual value. If UTF-8 support is required, do not use MySQL .","title":"Examples"},{"location":"getting-started/distributions/","text":"Dependency-Track has three distribution variants. They are: Package Package Format Recommended Supported Docker Download API Server Executable WAR Frontend Single Page Application Bundled Executable WAR API Server The API Server contains an embedded Jetty server and all server-side functionality, but excludes the frontend user interface. Frontend The Frontend is the user interface that is accessible in a web browser. The Frontend is a Single Page Application (SPA) that can be deployed independently of the Dependency-Track API Server. Bundled The Bundled variant combines the API Server and the Frontend user interface. This variant was previously referred to as the executable war and was the preferred distribution from Dependency-Track v3.0 - v3.8. Deprecation Notice This variant is supported but deprecated and will be discontinued in a future release.","title":"Distributions"},{"location":"getting-started/distributions/#api-server","text":"The API Server contains an embedded Jetty server and all server-side functionality, but excludes the frontend user interface.","title":"API Server"},{"location":"getting-started/distributions/#frontend","text":"The Frontend is the user interface that is accessible in a web browser. The Frontend is a Single Page Application (SPA) that can be deployed independently of the Dependency-Track API Server.","title":"Frontend"},{"location":"getting-started/distributions/#bundled","text":"The Bundled variant combines the API Server and the Frontend user interface. This variant was previously referred to as the executable war and was the preferred distribution from Dependency-Track v3.0 - v3.8. Deprecation Notice This variant is supported but deprecated and will be discontinued in a future release.","title":"Bundled"},{"location":"getting-started/initial-startup/","text":"Upon starting Dependency-Track for the first time, multiple tasks occur including: Generation of default objects such as users, teams, and permissions Generation of secret key used for JWT token creation and validation Population of CWE and SPDX license data Initial mirroring of all supported vulnerability datasources (National Vulnerability Database, GitHub Advisories, etc) Note The initial mirroring may take between 10 - 30 minutes or more. Do not interrupt this process. Wait for the completion of all mirroring tasks before shutting down the system. These tasks can be monitored by watching dependency-track.log or the Docker containers console. Default credentials An administrative account is created on initial startup with the following credentials: Username: admin Password: admin Upon first login, the admin user is required to change the password.","title":"Initial Startup"},{"location":"getting-started/initial-startup/#default-credentials","text":"An administrative account is created on initial startup with the following credentials: Username: admin Password: admin Upon first login, the admin user is required to change the password.","title":"Default credentials"},{"location":"getting-started/monitoring/","text":"Metrics Dependency-Track can be configured to expose system metrics using the Prometheus text-based exposition format . They can then be collected and visualized using tools like Prometheus and Grafana . To enable metrics exposition, set the alpine.metrics.enable property to true (see Configuration ). Docker Properties environment : ALPINE_METRICS_ENABLED : \"true\" alpine.metrics.enabled = true Warning Metrics will be exposed in the /metrics endpoint, which is not subject to access control. If protection is desired, it is recommended to add basic authentication at the reverse proxy or load balancer layer. A guide for NGINX can be found here . Grafana Dashboard Because Micrometer is used to collect and expose metrics, common Grafana dashboards for Micrometer should just work. An example Grafana dashboard is provided here . Refer to the Grafana documentation for instructions on how to import it. Info The example dashboard is meant to be a starting point. Users are strongly encouraged to explore the available metrics and build their own dashboards, tailored to their needs. The sample dashboard is not actively maintained by the project team, however community contributions are more than welcome.","title":"Monitoring"},{"location":"getting-started/monitoring/#metrics","text":"Dependency-Track can be configured to expose system metrics using the Prometheus text-based exposition format . They can then be collected and visualized using tools like Prometheus and Grafana . To enable metrics exposition, set the alpine.metrics.enable property to true (see Configuration ). Docker Properties environment : ALPINE_METRICS_ENABLED : \"true\" alpine.metrics.enabled = true Warning Metrics will be exposed in the /metrics endpoint, which is not subject to access control. If protection is desired, it is recommended to add basic authentication at the reverse proxy or load balancer layer. A guide for NGINX can be found here .","title":"Metrics"},{"location":"getting-started/monitoring/#grafana-dashboard","text":"Because Micrometer is used to collect and expose metrics, common Grafana dashboards for Micrometer should just work. An example Grafana dashboard is provided here . Refer to the Grafana documentation for instructions on how to import it. Info The example dashboard is meant to be a starting point. Users are strongly encouraged to explore the available metrics and build their own dashboards, tailored to their needs. The sample dashboard is not actively maintained by the project team, however community contributions are more than welcome.","title":"Grafana Dashboard"},{"location":"getting-started/deployment/docker/","text":"Deploying with Docker is the easiest and fastest method of getting started. No prerequisites are required other than a modern version of Docker. Info The latest tag in Docker Hub will always refer to the latest stable GA release. Consult the GitHub repo for instructions on how to run untested snapshot releases. Resource Requirements API Server Frontend Minimum Recommended RAM 4.5GB 16GB CPU 2 Cores 4 cores Minimum Recommended RAM 1GB 1GB CPU 1 Core 2 cores Quickstart Docker Compose Docker Swarm Manual Execution # Downloads the latest Docker Compose file curl -LO https://dependencytrack.org/docker-compose.yml # Starts the stack using Docker Compose docker-compose up -d # Downloads the latest Docker Compose file curl -LO https://dependencytrack.org/docker-compose.yml # Initializes Docker Swarm (if not previously initialized) docker swarm init # Starts the stack using Docker Swarm docker stack deploy -c docker-compose.yml dtrack # Pull the image from the Docker Hub OWASP repo docker pull dependencytrack/bundled # Creates a dedicated volume where data can be stored outside the container docker volume create --name dependency-track # Run the bundled container with 8GB RAM on port 8080 docker run -d -m 8192m -p 8080 :8080 --name dependency-track -v dependency-track:/data dependencytrack/bundled Docker Compose (Automated / Orchestration) The preferred method for production environments is to use docker-compose.yml with a corresponding database container (Postgres, MySQL, or Microsoft SQL). The following is an example YAML file that can be used with docker-compose or docker stack deploy . version : '3.7' ##################################################### # This Docker Compose file contains two services # Dependency-Track API Server # Dependency-Track FrontEnd ##################################################### volumes : dependency-track : services : dtrack-apiserver : image : dependencytrack/apiserver # environment: # The Dependency-Track container can be configured using any of the # available configuration properties defined in: # https://docs.dependencytrack.org/getting-started/configuration/ # All properties are upper case with periods replaced by underscores. # # Database Properties # - ALPINE_DATABASE_MODE=external # - ALPINE_DATABASE_URL=jdbc:postgresql://postgres10:5432/dtrack # - ALPINE_DATABASE_DRIVER=org.postgresql.Driver # - ALPINE_DATABASE_USERNAME=dtrack # - ALPINE_DATABASE_PASSWORD=changeme # - ALPINE_DATABASE_POOL_ENABLED=true # - ALPINE_DATABASE_POOL_MAX_SIZE=20 # - ALPINE_DATABASE_POOL_MIN_IDLE=10 # - ALPINE_DATABASE_POOL_IDLE_TIMEOUT=300000 # - ALPINE_DATABASE_POOL_MAX_LIFETIME=600000 # # Optional LDAP Properties # - ALPINE_LDAP_ENABLED=true # - ALPINE_LDAP_SERVER_URL=ldap://ldap.example.com:389 # - ALPINE_LDAP_BASEDN=dc=example,dc=com # - ALPINE_LDAP_SECURITY_AUTH=simple # - ALPINE_LDAP_BIND_USERNAME= # - ALPINE_LDAP_BIND_PASSWORD= # - ALPINE_LDAP_AUTH_USERNAME_FORMAT=%s@example.com # - ALPINE_LDAP_ATTRIBUTE_NAME=userPrincipalName # - ALPINE_LDAP_ATTRIBUTE_MAIL=mail # - ALPINE_LDAP_GROUPS_FILTER=(&(objectClass=group)(objectCategory=Group)) # - ALPINE_LDAP_USER_GROUPS_FILTER=(member:1.2.840.113556.1.4.1941:={USER_DN}) # - ALPINE_LDAP_GROUPS_SEARCH_FILTER=(&(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*)) # - ALPINE_LDAP_USERS_SEARCH_FILTER=(&(objectClass=user)(objectCategory=Person)(cn=*{SEARCH_TERM}*)) # - ALPINE_LDAP_USER_PROVISIONING=false # - ALPINE_LDAP_TEAM_SYNCHRONIZATION=false # # Optional OpenID Connect (OIDC) Properties # - ALPINE_OIDC_ENABLED=true # - ALPINE_OIDC_ISSUER=https://auth.example.com/auth/realms/example # - ALPINE_OIDC_USERNAME_CLAIM=preferred_username # - ALPINE_OIDC_TEAMS_CLAIM=groups # - ALPINE_OIDC_USER_PROVISIONING=true # - ALPINE_OIDC_TEAM_SYNCHRONIZATION=true # # Optional HTTP Proxy Settings # - ALPINE_HTTP_PROXY_ADDRESS=proxy.example.com # - ALPINE_HTTP_PROXY_PORT=8888 # - ALPINE_HTTP_PROXY_USERNAME= # - ALPINE_HTTP_PROXY_PASSWORD= # - ALPINE_NO_PROXY= # # Optional HTTP Outbound Connection Timeout Settings. All values are in seconds. # - ALPINE_HTTP_TIMEOUT_CONNECTION=30 # - ALPINE_HTTP_TIMEOUT_SOCKET=30 # - ALPINE_HTTP_TIMEOUT_POOL=60 # # Optional Cross-Origin Resource Sharing (CORS) Headers # - ALPINE_CORS_ENABLED=true # - ALPINE_CORS_ALLOW_ORIGIN=* # - ALPINE_CORS_ALLOW_METHODS=GET, POST, PUT, DELETE, OPTIONS # - ALPINE_CORS_ALLOW_HEADERS=Origin, Content-Type, Authorization, X-Requested-With, Content-Length, Accept, Origin, X-Api-Key, X-Total-Count, * # - ALPINE_CORS_EXPOSE_HEADERS=Origin, Content-Type, Authorization, X-Requested-With, Content-Length, Accept, Origin, X-Api-Key, X-Total-Count # - ALPINE_CORS_ALLOW_CREDENTIALS=true # - ALPINE_CORS_MAX_AGE=3600 # # Optional metrics properties # - ALPINE_METRICS_ENABLED=true # # Optional environmental variables to enable default notification publisher templates override and set the base directory to search for templates # - DEFAULT_TEMPLATES_OVERRIDE_ENABLED=false # - DEFAULT_TEMPLATES_OVERRIDE_BASE_DIRECTORY=/data deploy : resources : limits : memory : 12288m reservations : memory : 8192m restart_policy : condition : on-failure ports : - '8081:8080' volumes : # Optional volume mount to override default notification publisher templates # - \"/host/path/to/template/base/dir:/data/templates\" - 'dependency-track:/data' restart : unless-stopped dtrack-frontend : image : dependencytrack/frontend depends_on : - dtrack-apiserver environment : # The base URL of the API server. # NOTE: # * This URL must be reachable by the browsers of your users. # * The frontend container itself does NOT communicate with the API server directly, it just serves static files. # * When deploying to dedicated servers, please use the external IP or domain of the API server. - API_BASE_URL=http://localhost:8081 # - \"OIDC_ISSUER=\" # - \"OIDC_CLIENT_ID=\" # - \"OIDC_SCOPE=\" # - \"OIDC_FLOW=\" # - \"OIDC_LOGIN_BUTTON_TEXT=\" # volumes: # - \"/host/path/to/config.json:/app/static/config.json\" ports : - \"8080:8080\" restart : unless-stopped Bundled JDBC Drivers The following JDBC Drivers are included with Dependency-Track. Driver Class Microsoft SQL com.microsoft.sqlserver.jdbc.SQLServerDriver MySQL com.mysql.jdbc.Driver PostgreSQL org.postgresql.Driver","title":"Docker"},{"location":"getting-started/deployment/docker/#resource-requirements","text":"API Server Frontend Minimum Recommended RAM 4.5GB 16GB CPU 2 Cores 4 cores Minimum Recommended RAM 1GB 1GB CPU 1 Core 2 cores","title":"Resource Requirements"},{"location":"getting-started/deployment/docker/#quickstart","text":"Docker Compose Docker Swarm Manual Execution # Downloads the latest Docker Compose file curl -LO https://dependencytrack.org/docker-compose.yml # Starts the stack using Docker Compose docker-compose up -d # Downloads the latest Docker Compose file curl -LO https://dependencytrack.org/docker-compose.yml # Initializes Docker Swarm (if not previously initialized) docker swarm init # Starts the stack using Docker Swarm docker stack deploy -c docker-compose.yml dtrack # Pull the image from the Docker Hub OWASP repo docker pull dependencytrack/bundled # Creates a dedicated volume where data can be stored outside the container docker volume create --name dependency-track # Run the bundled container with 8GB RAM on port 8080 docker run -d -m 8192m -p 8080 :8080 --name dependency-track -v dependency-track:/data dependencytrack/bundled","title":"Quickstart"},{"location":"getting-started/deployment/docker/#docker-compose-automated-orchestration","text":"The preferred method for production environments is to use docker-compose.yml with a corresponding database container (Postgres, MySQL, or Microsoft SQL). The following is an example YAML file that can be used with docker-compose or docker stack deploy . version : '3.7' ##################################################### # This Docker Compose file contains two services # Dependency-Track API Server # Dependency-Track FrontEnd ##################################################### volumes : dependency-track : services : dtrack-apiserver : image : dependencytrack/apiserver # environment: # The Dependency-Track container can be configured using any of the # available configuration properties defined in: # https://docs.dependencytrack.org/getting-started/configuration/ # All properties are upper case with periods replaced by underscores. # # Database Properties # - ALPINE_DATABASE_MODE=external # - ALPINE_DATABASE_URL=jdbc:postgresql://postgres10:5432/dtrack # - ALPINE_DATABASE_DRIVER=org.postgresql.Driver # - ALPINE_DATABASE_USERNAME=dtrack # - ALPINE_DATABASE_PASSWORD=changeme # - ALPINE_DATABASE_POOL_ENABLED=true # - ALPINE_DATABASE_POOL_MAX_SIZE=20 # - ALPINE_DATABASE_POOL_MIN_IDLE=10 # - ALPINE_DATABASE_POOL_IDLE_TIMEOUT=300000 # - ALPINE_DATABASE_POOL_MAX_LIFETIME=600000 # # Optional LDAP Properties # - ALPINE_LDAP_ENABLED=true # - ALPINE_LDAP_SERVER_URL=ldap://ldap.example.com:389 # - ALPINE_LDAP_BASEDN=dc=example,dc=com # - ALPINE_LDAP_SECURITY_AUTH=simple # - ALPINE_LDAP_BIND_USERNAME= # - ALPINE_LDAP_BIND_PASSWORD= # - ALPINE_LDAP_AUTH_USERNAME_FORMAT=%s@example.com # - ALPINE_LDAP_ATTRIBUTE_NAME=userPrincipalName # - ALPINE_LDAP_ATTRIBUTE_MAIL=mail # - ALPINE_LDAP_GROUPS_FILTER=(&(objectClass=group)(objectCategory=Group)) # - ALPINE_LDAP_USER_GROUPS_FILTER=(member:1.2.840.113556.1.4.1941:={USER_DN}) # - ALPINE_LDAP_GROUPS_SEARCH_FILTER=(&(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*)) # - ALPINE_LDAP_USERS_SEARCH_FILTER=(&(objectClass=user)(objectCategory=Person)(cn=*{SEARCH_TERM}*)) # - ALPINE_LDAP_USER_PROVISIONING=false # - ALPINE_LDAP_TEAM_SYNCHRONIZATION=false # # Optional OpenID Connect (OIDC) Properties # - ALPINE_OIDC_ENABLED=true # - ALPINE_OIDC_ISSUER=https://auth.example.com/auth/realms/example # - ALPINE_OIDC_USERNAME_CLAIM=preferred_username # - ALPINE_OIDC_TEAMS_CLAIM=groups # - ALPINE_OIDC_USER_PROVISIONING=true # - ALPINE_OIDC_TEAM_SYNCHRONIZATION=true # # Optional HTTP Proxy Settings # - ALPINE_HTTP_PROXY_ADDRESS=proxy.example.com # - ALPINE_HTTP_PROXY_PORT=8888 # - ALPINE_HTTP_PROXY_USERNAME= # - ALPINE_HTTP_PROXY_PASSWORD= # - ALPINE_NO_PROXY= # # Optional HTTP Outbound Connection Timeout Settings. All values are in seconds. # - ALPINE_HTTP_TIMEOUT_CONNECTION=30 # - ALPINE_HTTP_TIMEOUT_SOCKET=30 # - ALPINE_HTTP_TIMEOUT_POOL=60 # # Optional Cross-Origin Resource Sharing (CORS) Headers # - ALPINE_CORS_ENABLED=true # - ALPINE_CORS_ALLOW_ORIGIN=* # - ALPINE_CORS_ALLOW_METHODS=GET, POST, PUT, DELETE, OPTIONS # - ALPINE_CORS_ALLOW_HEADERS=Origin, Content-Type, Authorization, X-Requested-With, Content-Length, Accept, Origin, X-Api-Key, X-Total-Count, * # - ALPINE_CORS_EXPOSE_HEADERS=Origin, Content-Type, Authorization, X-Requested-With, Content-Length, Accept, Origin, X-Api-Key, X-Total-Count # - ALPINE_CORS_ALLOW_CREDENTIALS=true # - ALPINE_CORS_MAX_AGE=3600 # # Optional metrics properties # - ALPINE_METRICS_ENABLED=true # # Optional environmental variables to enable default notification publisher templates override and set the base directory to search for templates # - DEFAULT_TEMPLATES_OVERRIDE_ENABLED=false # - DEFAULT_TEMPLATES_OVERRIDE_BASE_DIRECTORY=/data deploy : resources : limits : memory : 12288m reservations : memory : 8192m restart_policy : condition : on-failure ports : - '8081:8080' volumes : # Optional volume mount to override default notification publisher templates # - \"/host/path/to/template/base/dir:/data/templates\" - 'dependency-track:/data' restart : unless-stopped dtrack-frontend : image : dependencytrack/frontend depends_on : - dtrack-apiserver environment : # The base URL of the API server. # NOTE: # * This URL must be reachable by the browsers of your users. # * The frontend container itself does NOT communicate with the API server directly, it just serves static files. # * When deploying to dedicated servers, please use the external IP or domain of the API server. - API_BASE_URL=http://localhost:8081 # - \"OIDC_ISSUER=\" # - \"OIDC_CLIENT_ID=\" # - \"OIDC_SCOPE=\" # - \"OIDC_FLOW=\" # - \"OIDC_LOGIN_BUTTON_TEXT=\" # volumes: # - \"/host/path/to/config.json:/app/static/config.json\" ports : - \"8080:8080\" restart : unless-stopped","title":"Docker Compose (Automated / Orchestration)"},{"location":"getting-started/deployment/docker/#bundled-jdbc-drivers","text":"The following JDBC Drivers are included with Dependency-Track. Driver Class Microsoft SQL com.microsoft.sqlserver.jdbc.SQLServerDriver MySQL com.mysql.jdbc.Driver PostgreSQL org.postgresql.Driver","title":"Bundled JDBC Drivers"},{"location":"getting-started/deployment/exewar/","text":"An executable WAR is a traditional Java Web Archive (WAR) that is packaged in a way where it can executed from the command-line. Unlike traditional WARs which require a Servlet container already installed and configured, executable WARs fast-track this process by bundling a Servlet container specifically configured to execute the bundled application. The Dependency-Track executable WAR is delivered ready-to-run. To use the executable WAR, the only requirement is to have Java 8u162 (or higher) installed. Warning The Executable WAR is deprecated and will no longer be distributed in a future version of Dependency-Track It is advisable that organizations migrate to a container strategy such as Docker or Kubernetes. The Executable WAR is available in two variants: API Server Bundled Refer to distributions for details. Resource Requirements Minimum Recommended Java 11 11 RAM 4GB 16GB CPU 2 cores 4 cores If minimum requirements are not met, Dependency-Track will not start correctly. However, for systems with Java 8 already installed, this method of execution may provide the fastest deployment path. Startup # Executes Dependency-Track with default options java -Xmx8G -jar dependency-track-bundled.war Command-Line Arguments The following command-line arguments can be passed to a compiled executable WAR when executing it: Argument Default Description -context / The application context to deploy to -host 0.0.0.0 The IP address to bind to -port 8080 The TCP port to listens on Note: Setting the context is only supported on the API Server variant. The Frontend requires deployment to the root ('/') context. Examples java -Xmx12G -jar dependency-track-apiserver.war -context /dtrack java -Xmx12G -jar dependency-track-apiserver.war -port 8081 java -Xmx12G -jar dependency-track-apiserver.war -context /dtrack -host 192 .168.1.16 -port 9000 java -XX:MaxRAMPercentage = 80 .0 -jar dependency-track-bundled.war","title":"Executable WAR"},{"location":"getting-started/deployment/exewar/#resource-requirements","text":"Minimum Recommended Java 11 11 RAM 4GB 16GB CPU 2 cores 4 cores If minimum requirements are not met, Dependency-Track will not start correctly. However, for systems with Java 8 already installed, this method of execution may provide the fastest deployment path.","title":"Resource Requirements"},{"location":"getting-started/deployment/exewar/#startup","text":"# Executes Dependency-Track with default options java -Xmx8G -jar dependency-track-bundled.war","title":"Startup"},{"location":"getting-started/deployment/exewar/#command-line-arguments","text":"The following command-line arguments can be passed to a compiled executable WAR when executing it: Argument Default Description -context / The application context to deploy to -host 0.0.0.0 The IP address to bind to -port 8080 The TCP port to listens on Note: Setting the context is only supported on the API Server variant. The Frontend requires deployment to the root ('/') context.","title":"Command-Line Arguments"},{"location":"getting-started/deployment/exewar/#examples","text":"java -Xmx12G -jar dependency-track-apiserver.war -context /dtrack java -Xmx12G -jar dependency-track-apiserver.war -port 8081 java -Xmx12G -jar dependency-track-apiserver.war -context /dtrack -host 192 .168.1.16 -port 9000 java -XX:MaxRAMPercentage = 80 .0 -jar dependency-track-bundled.war","title":"Examples"},{"location":"getting-started/deployment/kubernetes/","text":"You can install on Kubernetes using the community-maintained chart like this: Helm v3 Helm v2 helm repo add evryfs-oss https://evryfs.github.io/helm-charts/ helm install dependency-track evryfs-oss/dependency-track --namespace dependency-track --create-namespace helm repo add evryfs-oss https://evryfs.github.io/helm-charts/ helm install evryfs-oss/dependency-track --name dependency-track --namespace dependency-track --create-namespace Note By default, it will install PostgreSQL and use persistent volume claims for the data directory used for vulnerability feeds.","title":"Kubernetes"}]}